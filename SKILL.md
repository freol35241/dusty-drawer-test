---
name: dusty-drawer-test
description: Reviews research project proposals from the viewpoint of impact. Use when evaluating whether to pursue a project, comparing project ideas, or assessing if a project is likely to end up "in a dusty drawer" vs. creating real change. Applies a structured impact framework with transparent reasoning at each step.
version: 1.0
---

# The Dusty Drawer Test

This skill evaluates research project proposals through an impact lens. It is designed to be fully transparent — not just providing scores, but explaining the concepts and reasoning behind each assessment.

The framework rests on seven foundational concepts: five about what impact *is*, two about how to pursue it sustainably.

---

# Part 1: Foundations

## What Impact Is

### 1.1 Counterfactual Thinking

**Core idea:** Impact means *additional* change — what happened that wouldn't have happened otherwise.

The question isn't "did good things happen?" but "did good things happen *because of this work*?" If the outcome would have occurred anyway — through other actors, other paths — then the counterfactual impact is zero regardless of quality.

**Application:** When evaluating a project, ask: would this change happen without it?

---

### 1.2 Pragmatism

**Core idea:** The meaning of an idea lies in its practical consequences.

An idea that changes nothing is not false — it's empty. Knowledge is a tool for navigating problems; its value lies in what it enables.

**Application:** A research output that no one uses, cites, or acts upon has no meaning by pragmatist standards. Impact requires consequence.

---

### 1.3 Systems Leverage

**Core idea:** Higher-impact interventions are harder to control.

Donella Meadows identified leverage points in systems: shallow interventions (parameters, constants) are easy but limited; deep interventions (goals, paradigms) are transformative but hard to execute.

The paradox: the most powerful interventions are the least controllable.

**Application:** There's an inverse relationship between impact magnitude and personal control. This is structural, not a failure.

---

### 1.4 Neglectedness

**Core idea:** Marginal impact is highest where few others are working.

The ITN framework: Importance × Tractability × Neglectedness ≈ Marginal impact

A crowded field has diminishing returns. The first researcher on a problem contributes more than the hundredth.

**Application:** "Is this important?" is the wrong question. Ask: "What's the value of *one more* person working on it?"

---

### 1.5 Theory of Change

**Core idea:** Impact requires a causal pathway — a chain of "if-then" assumptions connecting output to ultimate change.

A Theory of Change makes explicit:
- What you will produce (outputs)
- What must happen for that to lead to change (assumptions)
- What intermediate changes occur (outcomes)
- What ultimate difference results (impact)

Each "if-then" link can fail. "If we publish this, then decision-makers will read it. If they read it, they will understand it. If they understand, they will act." Each step is an assumption.

**Application:** Trace the causal path. Name each assumption. The more links, the more fragile the chain.

---

## The Human Side

### 1.6 Task Separation

**Core idea:** Distinguish what you control from what others control — and relate to each differently.

Alfred Adler's psychology:
- **Your task:** What you control (quality, shipping, your learning)
- **Others' task:** Their responses (engagement, adoption, citation)

Confusing the two causes suffering — you feel responsible for things you cannot control.

**Application:** Do your task well; release attachment to others' tasks. This isn't indifference — it's directing energy where it makes a difference.

---

### 1.7 Feedback Loops

**Core idea:** Long feedback loops erode motivation and prevent course correction.

Self-efficacy — belief that your actions produce results — requires feedback. Without it, motivation erodes toward learned helplessness.

Research often has brutal feedback loops. You may not know for years whether work mattered.

**Application:** Design for intermediate feedback. Maintain work at multiple time horizons — short-loop work sustains motivation for long-term bets.

---

# Part 2: The Framework

## Core Definition

**Impact = someone did something they otherwise wouldn't have.**

---

## The Impact Ladder

Eight levels, each requiring the previous:

| Level | What it means | Your control |
|-------|---------------|--------------|
| 1. Done | You finished | High |
| 2. Shipped | Others can access it | High |
| 3. Attention | People encountered it | Medium |
| 4. Understanding | People learned something | Medium |
| 5. Conversation shift | Your framing enters discourse | Medium-Low |
| 6. Decision input | Someone referenced it in a choice | Low |
| 7. Behavior change | Actions changed | Low |
| 8. Systemic change | The system works differently | Very Low |

Control decreases as impact increases. Task separation is the psychological response.

---

## Pulled, Revealed, and Pushed Work

| Type | Definition | Dusty-drawer risk |
|------|------------|-------------------|
| **Pulled** | Someone needs it, is waiting | Low |
| **Revealed** | No one asked, but they'll recognize value | Medium |
| **Pushed** | You made it, hope someone cares | High |

What creates pull: pain, money, obligation, community.

The difference between revealed and pushed: revealed work discloses something true that others will recognize. Pushed work rests on belief that others *should* care.

---

## Evaluation Questions

### Impact Potential

#### 2.1 The Who Question

**Foundation:** Counterfactual thinking

**Ask:** "Who will carry this work up the ladder?"

Research outputs rarely reach end users directly. Someone has to pick up your work and do something with it: implement your method, deploy your tool, translate your finding into practice. This intermediary is the critical link. If you can't name them, your work stops at "shipped."

**Evaluate:**
- Can you name the specific person or role who will take your output and act on it?
- Not "industry" or "researchers," but which team, which company, which decision-maker?
- Do you have a relationship with them, or are you hoping they'll find you?
- Why would they invest effort in carrying your work forward?

**Scoring:**
- 0: Cannot identify a concrete carrier; relying on "someone will find this useful"
- 1: Can identify categories or types of actors, but no specific names or relationships
- 2: Can name specific people/organizations who will carry this forward, with existing relationship or clear path to one

**Transparency requirement:** Name the carrier explicitly. If you can only name end beneficiaries but not who will bring your work to them, acknowledge this gap.

---

#### 2.2 The What Change Question

**Foundation:** Pragmatism

**Ask:** "What will they do differently, and how direct is the path from your output to that change?"

**Types of change (from most to least concrete):**
- A decision: choose A over B
- A behavior: operate differently day-to-day
- A capability: can now do what they couldn't
- An understanding: see something new

**Indirection matters.** Count the steps between your output and the change:
- Direct (0 steps): Your tool is the thing they use to act differently
- Once removed (1 step): Someone must implement/translate your work first
- Twice removed (2+ steps): Multiple actors must act before change happens

Each step of indirection is a point where the chain can break.

**Scoring:**
- 0: Cannot articulate concrete change, or change is 3+ steps removed
- 1: Can articulate change but it's abstract ("better decisions") or 2 steps removed
- 2: Concrete, observable change that is direct or once removed at most

**Transparency requirement:** Describe the specific change. Count the steps of indirection explicitly. "They will optimize routes" means nothing without specifying: are you building the optimizer, or writing a paper about optimization that someone else might read and then build something?

---

#### 2.3 The Causal Path Question

**Foundation:** Theory of Change

**Ask:** "What must be true for your work to cause this change?"

**Evaluate:**
- List the "if-then" assumptions in the causal chain
- How many links? How fragile is each?
- Which assumptions are testable?

**Scoring:**
- 0: Many fragile links; long chain with untested assumptions
- 1: Some assumptions; moderate chain length
- 2: Short, robust chain; assumptions are plausible or testable

**Transparency requirement:** List the causal chain explicitly. Name each assumption. Identify the weakest links.

---

#### 2.4 The Fit Question

**Foundation:** Neglectedness

**Ask:** "Why you/your team for this problem?"

**Consider:**
- Domain expertise match
- Technical skill match
- Access, relationships, or data that create advantage
- Why this person/team vs. anyone else?

**Scoring:**
- 0: No particular advantage; others equally positioned
- 1: Partial fit; some skills match, some gaps
- 2: Strong fit; clear competitive advantage

**Transparency requirement:** Name specific advantages. Identify gaps. Assess whether marginal contribution is meaningful.

---

#### 2.5 The Values Question

**Ask:** "Does this leave things better?"

**Evaluate:**
1. Identify the ultimate beneficiary
2. Trace the causal chain from output to benefit
3. Count steps of indirection
4. Assess net effect

**Scoring:**
- 0: Neutral, extractive, or potentially harmful
- 1: Indirect benefit, multiple steps removed
- 2: Direct alignment with clear positive effect

**Transparency requirement:** Trace the path from output to value. Be honest about indirection.

---

### Sustainability

#### 2.6 The Feedback Question

**Foundation:** Feedback loops

**Ask:** "How long until you know if it worked?"

**Timeframes:**
- Days to weeks: Strong signal, sustainable
- Months: Acceptable if intermediate checkpoints exist
- Years or never: Only sustainable if balanced with shorter-loop work

**Scoring:**
- 0: Years, or no clear feedback mechanism
- 1: Months, with checkpoints possible
- 2: Weeks, with clear validation opportunities

**Transparency requirement:** Identify specific moments when feedback arrives. Assess psychological sustainability.

---

#### 2.7 The Control Question

**Foundation:** Task separation

**Ask:** "What's yours to control, what isn't?"

**Separate:**
- Proposer's task: quality, shipping, learning
- Others' task: engagement, adoption, downstream effects

**Scoring:**
- 0: Success depends entirely on others' responses
- 1: Can ship something concrete, but value requires adoption
- 2: Concrete value regardless of external response

**Transparency requirement:** Name what can be shipped regardless of others. Assess whether the proposer can find meaning even if it doesn't ripple outward.

---

# Part 3: Classification

After scoring, classify:

**Pull type:** Pulled / Revealed / Pushed
→ Indicates dusty-drawer risk

**Ladder ceiling:** Which level is realistically achievable?
→ Indicates control boundary

---

# Part 4: Synthesis

## Score Summary

| Criterion | Score (0-2) | Key reasoning |
|-----------|-------------|---------------|
| Who | | [One sentence] |
| What change | | [One sentence] |
| Causal path | | [One sentence] |
| Fit | | [One sentence] |
| Values | | [One sentence] |
| Feedback | | [One sentence] |
| Control | | [One sentence] |
| **Total** | **/14** | |

## Interpretation

- **Below 7:** Likely to struggle. Significant reframing needed, or consider passing.
- **7-10:** Proceed with eyes open. Address identified weaknesses.
- **11-14:** Strong candidate. Protect this work.

## Classification Summary

| Lens | Assessment | Implication |
|------|------------|-------------|
| Pull type | Pulled / Revealed / Pushed | [Risk level] |
| Ladder ceiling | [Level] | [Control boundary] |

## Strengths

List 2-3 specific strengths with reasoning connecting to foundations.

## Weaknesses and Risks

List 2-3 specific weaknesses. Include:
- Dusty-drawer risks (from pull classification)
- Control risks (from task separation)
- Feedback risks (sustainability)
- Causal chain fragility (from Theory of Change)

## Recommendations

- **If proceeding:** How to strengthen weak areas; what to monitor
- **If reconsidering:** What would need to change; alternative framings
- **If passing:** Why, and what type of project would be better

---

# Part 5: Applying the Framework

## Goodhart's Law

"When a measure becomes a target, it ceases to be a good measure."

**This framework is not immune.** The scores can become targets. You might adjust framing to score higher rather than to improve the project.

**The discipline:** Notice when this happens. The goal is honest assessment, not high numbers. If you're gaming the rubric, step back.

---

## Uncertainty

All assessments in this framework are guesses.

You don't *know* who will act differently — you're predicting. You don't *know* the causal path will hold — you're assuming. The 0/1/2 scores feel precise but aren't.

**The discipline:** Hold conclusions loosely. A score of 11/14 means "based on current guesses, this looks promising" — not "good project." New information should update assessment.

---

## Purpose

This framework surfaces what might remain implicit — so proposers can make informed choices about where to invest finite effort. It is a thinking tool, not a gatekeeping mechanism.
